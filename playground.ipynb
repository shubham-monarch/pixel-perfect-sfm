{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5983c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and refine it with the featuremetric optimization. We then localize an image downloaded from the Internet and show the effect of the refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c6f36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We start by defining some output paths: where the intermediate files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hloc import extract_features, match_features, reconstruction, pairs_from_exhaustive, visualization\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "from pixsfm.util.visualize import init_image, plot_points2D\n",
    "from pixsfm.refine_hloc import PixSfM\n",
    "from pixsfm import ostream_redirect\n",
    "from PIL import Image, ImageDraw\n",
    "import pycolmap\n",
    "from pathlib import Path\n",
    "#import visualize_model\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b577c8a-72ac-48da-aac2-c4c2bd22c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/monarch/')\n",
    "outputs = Path('outputs/monarch-demo/')\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "raw_dir = outputs / \"raw\"\n",
    "ref_dir = outputs / \"ref\"\n",
    "''' model location in case of intrinsics locked '''\n",
    "ref_dir_locked = outputs / \"ref_locked\"\n",
    "''' model location in case of intrinsics not locked '''\n",
    "ref_dir_not_locked = outputs / \"ref_dir_not_locked\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_axis(p: np.ndarray):\n",
    "    \n",
    "# Original point\n",
    "    \n",
    "    # Normalize p to get the unit vector\n",
    "    p_unit = p / np.linalg.norm(p)\n",
    "\n",
    "    # Unit vector in the x direction\n",
    "    x_unit = np.array([1, 0, 0])\n",
    "\n",
    "    # Cross product of p_unit and x_unit\n",
    "    v = np.cross(p_unit, x_unit)\n",
    "\n",
    "    # Sine and cosine of the angle between p_unit and x_unit\n",
    "    s = np.linalg.norm(v)\n",
    "    c = np.dot(p_unit, x_unit)\n",
    "\n",
    "    # Skew-symmetric cross-product matrix of v\n",
    "    Vx = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n",
    "\n",
    "    # Rotation matrix\n",
    "    R = np.eye(3) + Vx + np.dot(Vx, Vx) * ((1 - c) / (s ** 2))\n",
    "\n",
    "    # New point\n",
    "    p_new = np.dot(R, p)\n",
    "\n",
    "    return p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3631a2",
   "metadata": {},
   "source": [
    "### Helper functions for relative pose calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708a98e-2ea9-4808-bab6-a31983e21060",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "e_lw => left camera pose in world frame (4 * 4)\n",
    "e_rw => right camera pose in world frame (4 * 4)\n",
    "'''\n",
    "from scipy.spatial.transform import Rotation\n",
    "    \n",
    "\n",
    "def transformation_matrix_to_arr(T : np.ndarray):\n",
    "    R  = T[:3,:3] #extracting the rotation matrix\n",
    "    dx = T[0,3]\n",
    "    dy = T[1,3]\n",
    "    dz = T[2,3]\n",
    "    dquat = Rotation.from_matrix(R).as_quat()\n",
    "    rel_pose_arr = [dx,dy,dz]\n",
    "    for q in dquat: \n",
    "        rel_pose_arr.append(q)\n",
    "    return rel_pose_arr\n",
    "\n",
    "def relative_pose_transformation_matrix(e_lw: np.ndarray, e_rw: np.ndarray):\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    e_wl = np.linalg.inv(e_lw)\n",
    "    e_rl = np.dot(e_rw,np.linalg.inv(e_lw))\n",
    "    return e_rl\n",
    "\n",
    "#def calculate_relative_pose(e_lw, e_rw):\n",
    "def calculate_relative_pose(e_lw: np.ndarray, e_rw: np.ndarray):\n",
    "    #print(f\"Inside the calculate_relative_pose function\")\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    e_wl = np.linalg.inv(e_lw)\n",
    "    #print(f\"e_wl: {e_wl}\")\n",
    "    #e_rl = e_rw * np.linalg.inv(e_lw) #right camera in the frame of the left camera\n",
    "    #e_rl = e_rw * e_wl #right camera in the frame of the left camera\n",
    "    #print(f\"e_rl: {e_rl}\")\n",
    "    e_rl = np.dot(e_rw,np.linalg.inv(e_lw))\n",
    "    R = e_rl[:3,:3] #extracting the rotation matrix\n",
    "    dx = e_rl[0,3]\n",
    "    dy = e_rl[1,3]\n",
    "    dz = e_rl[2,3]\n",
    "    dquat = Rotation.from_matrix(R).as_quat()\n",
    "    #rel_pose =  [dx, dy] + dquat\n",
    "    rel_pose = [dx,dy,dz]\n",
    "    for q in dquat: \n",
    "        rel_pose.append(q)\n",
    "    return rel_pose\n",
    "    #return [dx,dy]\n",
    "    #print(f\"dx: {dx} dy: {dy} dquat: {dquat}\")\n",
    "    \n",
    "\n",
    "def cam_extrinsics(img):\n",
    "    from read_write_model import qvec2rotmat\n",
    "    R = qvec2rotmat(img.qvec)\n",
    "    t = img.tvec.reshape(3,-1)\n",
    "    #print(f\"R: {R} t: {t}\")\n",
    "    R_t = np.concatenate((R,t), axis = 1)\n",
    "    #R_t = np.vstack([np.array([0,0,0,1]), R_t])\n",
    "    R_t = np.vstack([R_t, np.array([0,0,0,1])])\n",
    "    return R_t    #  4 * 4 matrix\n",
    "\n",
    "def calculate_relative_pose_between(img_dict: dict, left_idx: int, right_idx: int):\n",
    "    left_img = img_dict[left_idx]\n",
    "    right_img = img_dict[right_idx]\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    return rel_pose\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbcfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_baseline_axis(T: np.ndarray):\n",
    "\n",
    "    # Extract the translation vector\n",
    "    t = T[:3, 3]\n",
    "\n",
    "    # Calculate the magnitude of the translation\n",
    "    t_mag = np.linalg.norm(t)\n",
    "\n",
    "    # Calculate the rotation angles\n",
    "    theta_y = np.arctan2(t[2], t[0])  # Rotation around y-axis\n",
    "    theta_z = -np.arctan2(t[1], np.sqrt(t[0]**2 + t[2]**2))  # Rotation around z-axis\n",
    "\n",
    "    # Create the rotation matrices\n",
    "    Ry = np.array([\n",
    "        [np.cos(theta_y), 0, np.sin(theta_y), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [-np.sin(theta_y), 0, np.cos(theta_y), 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    Rz = np.array([\n",
    "        [np.cos(theta_z), -np.sin(theta_z), 0, 0],\n",
    "        [np.sin(theta_z), np.cos(theta_z), 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])\n",
    "\n",
    "    # Apply the rotations to the transformation matrix\n",
    "    T_new = np.dot(Rz, np.dot(Ry, T))\n",
    "\n",
    "    # Set the translation vector to its magnitude in the x direction\n",
    "    T_new[:3, 3] = [t_mag, 0, 0]\n",
    "    \n",
    "    return T_new\n",
    "    #print(T_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77860966-409a-45fb-9df0-28f0a3794f69",
   "metadata": {},
   "source": [
    "### Camera positions WITHOUT Rig Bundle Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b074e-76b1-480c-af86-c4aa3ddefff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "sparse_dir = ref_dir_locked \n",
    "print(f\"sparse_dir: {sparse_dir.as_posix()}\")\n",
    "sparse_images = sparse_dir / \"images.bin\"\n",
    "sparse_points3D = sparse_dir / \"points3D.bin\"\n",
    "sparse_cameras = sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_write_model import read_cameras_binary\n",
    "cameras = read_cameras_binary(sparse_cameras)\n",
    "print(cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4122ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.expandvars('$HOME/colmap/scripts/python'))\n",
    "#sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "from read_write_model import read_images_binary \n",
    "sparse_img_dict = read_images_binary(sparse_images)\n",
    "print(f\"{len(sparse_img_dict.keys())} ==> {sparse_img_dict.keys()}\")\n",
    "print(f\"min_key: {min(sparse_img_dict.keys())} mx_key: {max(sparse_img_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633f78d-7fba-4d8d-8ab1-cd445552e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rel_poses = []\n",
    "rel_poses_aligned = []\n",
    "num_images = len(sparse_img_dict.keys())\n",
    "for idx in range(1, num_images // 2 + 1):\n",
    "    left_img = sparse_img_dict[idx]\n",
    "    right_img = sparse_img_dict[idx + 42]\n",
    "    #print(f\"left_img_name: {left_img.name} right_img_name: {right_img.name}\")\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    #unaligned relative pose calculation\n",
    "    e_rl = calculate_relative_pose(e_lw, e_rw)\n",
    "    rel_poses.append(e_rl)\n",
    "    #aligned relative pose calculation\n",
    "    e_rl_transformation_matrix = relative_pose_transformation_matrix(e_lw, e_rw)\n",
    "    e_rl_aligned_transformation_matrix = align_baseline_axis(e_rl_transformation_matrix)\n",
    "    #print(type(e_rl_aligned_transformation_matrix))\n",
    "    e_rl_aligned_transformation_arr = transformation_matrix_to_arr(e_rl_aligned_transformation_matrix)\n",
    "    rel_poses_aligned.append(e_rl_aligned_transformation_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0adc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(rel_poses): {type(rel_poses)}\")\n",
    "print(f\"rel_poses[0]: {rel_poses[0]} type(rel_poses[0]): {type(rel_poses[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34d636-d470-4273-8c9c-258d08e313a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cc0fa-25ee-4136-942d-3a9422a15913",
   "metadata": {},
   "source": [
    "### Camera poses with Rig Bundle Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ceb78-de4c-4db3-bdfb-5cb8add43f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rig_ba_sparse_dir = Path(os.path.expandvars('$HOME/rig_stereo_colmap_cli_output'))\n",
    "rig_ba_sparse_images = rig_ba_sparse_dir / \"images.bin\"\n",
    "rig_ba_sparse_points3D = rig_ba_sparse_dir / \"points3D.bin\"\n",
    "rig_ba_sparse_cameras = rig_ba_sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f269c61-bfc7-41f8-8dac-1148628ec948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "sys.path.append(os.path.expandvars('$HOME/colmap/scripts/python'))\n",
    "from read_write_model import read_images_binary \n",
    "rig_ba_sparse_img_dict = read_images_binary(rig_ba_sparse_images)\n",
    "print(f\"{len(rig_ba_sparse_img_dict.keys())} => {rig_ba_sparse_img_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f38e7a-2547-4b16-b6a2-a25dee4b71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rel_poses = []\n",
    "rel_poses_aligned = [] \n",
    "num_images = len(sparse_img_dict.keys())\n",
    "\n",
    "for idx in range(1, num_images // 2 + 1):\n",
    "    left_img = rig_ba_sparse_img_dict[idx]\n",
    "    right_img = rig_ba_sparse_img_dict[idx + 42]\n",
    "    #print(f\"left_img_name: {left_img.name} right_img_name: {right_img.name}\")\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    \n",
    "    #unaligned relative pose calculation\n",
    "    e_rl = calculate_relative_pose(e_lw, e_rw)\n",
    "    rel_poses.append(e_rl)\n",
    "    \n",
    "    #aligned relative pose calculation\n",
    "    e_rl_transformation_matrix = relative_pose_transformation_matrix(e_lw, e_rw)\n",
    "    e_rl_aligned_transformation_matrix = align_baseline_axis(e_rl_transformation_matrix)\n",
    "    e_rl_aligned_transformation_arr = transformation_matrix_to_arr(e_rl_aligned_transformation_matrix)\n",
    "    rel_poses_aligned.append(e_rl_aligned_transformation_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "rig_ba_rel_poses = []\n",
    "num_images = len(rig_ba_sparse_img_dict.keys())\n",
    "for idx in range(1, num_images // 2 + 1):\n",
    "    left_img = rig_ba_sparse_img_dict[idx]\n",
    "    right_img = rig_ba_sparse_img_dict[idx + 42]\n",
    "    if idx < 5:\n",
    "        print(f\"left_img_name: {left_img.name} right_img_name: {right_img.name}\")\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    rig_ba_rel_poses.append(rel_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45309f8b-af8f-417b-b928-a44ab79c30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897091f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_write_model import read_cameras_binary\n",
    "cameras = read_cameras_binary(rig_ba_sparse_cameras)\n",
    "print(cameras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
