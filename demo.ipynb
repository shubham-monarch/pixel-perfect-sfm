{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5983c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and refine it with the featuremetric optimization. We then localize an image downloaded from the Internet and show the effect of the refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "\n",
    "from hloc import extract_features, match_features, reconstruction, pairs_from_exhaustive, visualization\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "\n",
    "from pixsfm.util.visualize import init_image, plot_points2D\n",
    "from pixsfm.refine_hloc import PixSfM\n",
    "from pixsfm import ostream_redirect\n",
    "\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c6f36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We start by defining some output paths: where the intermediate files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab294072",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/sacre_coeur')\n",
    "outputs = Path('outputs/demo/')\n",
    "!rm -rf $outputs\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "raw_dir = outputs / \"raw\"\n",
    "ref_dir = outputs / \"ref\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bef9e3",
   "metadata": {},
   "source": [
    "Here we will use SuperPoint local features with the SuperGlue matcher, but it's easy to switch to other features like SIFT or R2D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4182c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29335da0",
   "metadata": {},
   "source": [
    "# 3D mapping and refinement\n",
    "First we list the images used for mapping. These are all day-time shots of Sacre Coeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [str(p.relative_to(images)) for p in (images / 'mapping/').iterdir()]\n",
    "print(len(references), \"mapping images\")\n",
    "plot_images([read_image(images / r) for r in references[:4]], dpi=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e86f9d",
   "metadata": {},
   "source": [
    "Then we extract features and match them across image pairs. Since we deal with few images, we simply match all pairs exhaustively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21540f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_features.main(feature_conf, images, image_list=references, feature_path=features)\n",
    "pairs_from_exhaustive.main(sfm_pairs, image_list=references)\n",
    "match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232823a",
   "metadata": {},
   "source": [
    "Now we run the reconstruction with and without the featuremetric refinement. For this dataset, when computing the dense features, we resize the images such that they are not larger than 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run pixsfm\n",
    "sfm = PixSfM({\"dense_features\": {\"max_edge\": 1024}})\n",
    "refined, sfm_outputs = sfm.reconstruction(ref_dir, images, sfm_pairs, features, matches, image_list=references)\n",
    "# here ref is pycolmap.Reconstruction object\n",
    "\n",
    "# run the raw geometric SfM for comparison\n",
    "raw_sfm = PixSfM({\"KA\":{\"apply\": False}, \"BA\": {\"apply\": False}})\n",
    "raw, _ = raw_sfm.reconstruction(raw_dir, images, sfm_pairs, features, matches, image_list=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0cf70e",
   "metadata": {},
   "source": [
    "`refined` and `raw` are `pycolmap.Reconstruction` objects, of which we can print the statistics to compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw\", raw.summary())\n",
    "print(\"Refined\", refined.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bd7d3",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "To visualize both models together, we rigidly transform the raw reconstruction so that the 3D points common to the two 3D models are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.align_points(refined, max_error=0.005, min_inlier_ratio=0.9, min_overlap=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ef122",
   "metadata": {},
   "source": [
    "We now plot the reconstructions side-by-side. We can click on the legend entries to toggle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56439fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3d = init_figure()\n",
    "args = dict(max_reproj_error=3.0, min_track_length=2, cs=1)\n",
    "plot_reconstruction(fig3d, raw, color='rgba(255, 0, 0, 0.5)', name=\"raw\", **args)\n",
    "plot_reconstruction(fig3d, refined, color='rgba(0, 255, 0, 0.5)', name=\"refined\", **args)\n",
    "fig3d.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f6aa9",
   "metadata": {},
   "source": [
    "We can also visualize the detected keypoints (blue) and the final reprojections (red) for a given mapping image. You can drag to zoom in. As you can see, the points were refined by a few pixels at most but the 3D points and camera poses can be refined up to a few meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc46954",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = refined.images[refined.reg_image_ids()[0]]\n",
    "cam = refined.cameras[img.camera_id]\n",
    "fig = init_image(images / img.name)    \n",
    "plot_points2D(fig, [p2D.xy for p2D in img.points2D if p2D.has_point3D()])\n",
    "plot_points2D(fig, cam.world_to_image(img.project(refined)), color='rgba(255, 0, 0, 0.5)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57fbbf",
   "metadata": {},
   "source": [
    "# Localization\n",
    "Now that we have a 3D map of the scene, we can localize any image. To demonstrate this, we download [a night-time image from Wikimedia](https://commons.wikimedia.org/wiki/File:Paris_-_Basilique_du_Sacr%C3%A9_Coeur,_Montmartre_-_panoramio.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/5/53/Paris_-_Basilique_du_Sacr%C3%A9_Coeur%2C_Montmartre_-_panoramio.jpg\"\n",
    "# try other queries by uncommenting their url\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/8/8e/Sacr%C3%A9_C%C5%93ur_at_night%21_%285865355326%29.jpg\"\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/c/c0/La_basilique_du_Sacr%C3%A9-Coeur_au_cr%C3%A9puscule_%28Paris%29_%284147593805%29.jpg\"\n",
    "query = 'query/night.jpg'\n",
    "!mkdir -p $images/query && wget $url -O $images/$query -q\n",
    "plot_images([read_image(images / query)], dpi=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854edc74",
   "metadata": {},
   "source": [
    "Again, we extract features for the query and match them exhaustively with all mapping images that were successfully reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_registered = [refined.images[i].name for i in refined.reg_image_ids()]\n",
    "extract_features.main(feature_conf, images, image_list=[query], feature_path=features, overwrite=True)\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=[query], ref_list=references_registered)\n",
    "match_features.main(matcher_conf, loc_pairs, features=features, matches=matches, overwrite=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa75646",
   "metadata": {},
   "source": [
    "We read the EXIF data of the query to infer a rough initial estimate of camera parameters like the focal length. Then we estimate the absolute camera pose using PnP+RANSAC and refine the camera parameters. Under the hood, the `QueryLocalizer` takes care of extracting dense features for the query and runs the keypoint and pose adjustments, QKA and QBA. The refinement refines the camera parameters in-place so we can inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycolmap\n",
    "from pixsfm.localize import QueryLocalizer, pose_from_cluster\n",
    "\n",
    "camera = pycolmap.infer_camera_from_image(images / query)\n",
    "ref_ids = [refined.find_image_with_name(r).image_id for r in references_registered]\n",
    "conf = {\n",
    "    \"dense_features\": sfm.conf.dense_features,  # same features as the SfM refinement\n",
    "    \"PnP\": {  # initial pose estimation with PnP+RANSAC\n",
    "        'estimation': {'ransac': {'max_error': 12.0}},\n",
    "        'refinement': {'refine_focal_length': True, 'refine_extra_params': True},\n",
    "    },\n",
    "    \"QBA\": {  # query pose refinement\n",
    "        \"optimizer:\": {'refine_focal_length': True, 'refine_extra_params': True},\n",
    "    }\n",
    "}\n",
    "dense_features = sfm_outputs[\"feature_manager\"]\n",
    "localizer = QueryLocalizer(refined, conf, dense_features=dense_features)\n",
    "ret, log = pose_from_cluster(localizer, query, camera, ref_ids, features, matches, image_path=images/query)\n",
    "\n",
    "print(f'found {sum(ret[\"inliers\"])}/{len(ret[\"inliers\"])} inlier correspondences.')\n",
    "visualization.visualize_loc_from_log(images, query, log, refined, top_k_db=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f23aa",
   "metadata": {},
   "source": [
    "We visualize the correspondences between the query images a few mapping images. We can also visualize the estimated camera pose in the 3D map, shown here in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558eb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pycolmap.Image(tvec=ret['tvec'], qvec=ret['qvec'])\n",
    "plot_camera_colmap(fig3d, pose, camera, color='rgba(128,128,255,0.5)', name=query, legendgroup=\"refined\")\n",
    "fig3d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
