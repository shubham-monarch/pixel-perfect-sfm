{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5983c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and refine it with the featuremetric optimization. We then localize an image downloaded from the Internet and show the effect of the refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c6f36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We start by defining some output paths: where the intermediate files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from hloc import extract_features, match_features, reconstruction, pairs_from_exhaustive, visualization\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "\n",
    "from pixsfm.util.visualize import init_image, plot_points2D\n",
    "from pixsfm.refine_hloc import PixSfM\n",
    "from pixsfm import ostream_redirect\n",
    "from PIL import Image, ImageDraw\n",
    "import pycolmap\n",
    "import numpy as np\n",
    "#import visualize_model\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b577c8a-72ac-48da-aac2-c4c2bd22c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab294072",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/monarch/')\n",
    "outputs = Path('outputs/monarch-demo/')\n",
    "!rm -rf $outputs\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "raw_dir = outputs / \"raw\"\n",
    "ref_dir = outputs / \"ref\"\n",
    "''' model location in case of intrinsics locked '''\n",
    "ref_dir_locked = outputs / \"ref_locked\"\n",
    "''' model location in case of intrinsics not locked '''\n",
    "ref_dir_not_locked = outputs / \"ref_dir_not_locked\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bef9e3",
   "metadata": {},
   "source": [
    "Here we will use SuperPoint local features with the SuperGlue matcher, but it's easy to switch to other features like SIFT or R2D2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc286cf",
   "metadata": {},
   "source": [
    "### Analysing Sparse Pointcloud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4768fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_dir = Path(\"/home/skumar/stereo_colmap_cli_output/sparse/\")\n",
    "print(f\"sparse_dir: {sparse_dir.as_posix()}\")\n",
    "sparse_images = sparse_dir / \"images.bin\"\n",
    "sparse_points3D = sparse_dir / \"points3D.bin\"\n",
    "sparse_cameras = sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7293679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = pycolmap.Reconstruction()\n",
    "sparse_model.read_binary(sparse_dir.as_posix())\n",
    "print(f\"sparse_model.summary(): {sparse_model.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51fd20",
   "metadata": {},
   "source": [
    "### Baseline check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "from read_write_model import read_images_binary \n",
    "sparse_img_dict = read_images_binary(sparse_images)\n",
    "print(sparse_img_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_extrinsics(img):\n",
    "    from read_write_model import qvec2rotmat\n",
    "    R = qvec2rotmat(img.qvec)\n",
    "    t = img.tvec.reshape(3,-1)\n",
    "    R_t = np.concatenate((R,t), axis = 1)\n",
    "    R_t = np.vstack([np.array([0,0,0,1]), R_t])\n",
    "    return R_t    #  4 * 4 matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d23781",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "e_lw => left camera pose in world frame (4 * 4)\n",
    "e_rw => right camera pose in world frame (4 * 4)\n",
    "'''\n",
    "def calculate_relative_pose(e_lw, e_rw): \n",
    "    from scipy.spatial.transform import Rotation\n",
    "    e_rl = e_rw * np.linalg.inv(e_lw) #right camera in the frame of the left camera\n",
    "    R = e_rl[:3,:3] #extracting the rotation matrix\n",
    "    dx = e_rl[0,3]\n",
    "    dy = e_rl[1,3]\n",
    "    dz = e_rl[2,3]\n",
    "    dquat = Rotation.from_matrix(R).as_quat()\n",
    "    #rel_pose =  [dx, dy] + dquat\n",
    "    rel_pose = [dx,dy,dz]\n",
    "    for q in dquat: \n",
    "        rel_pose.append(q)\n",
    "    return rel_pose\n",
    "    #return [dx,dy]\n",
    "    #print(f\"dx: {dx} dy: {dy} dquat: {dquat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57333f-c5dd-4800-8c5c-41bccec93eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(sparse_images): {type(sparse_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_poses = []\n",
    "num_images = len(sparse_img_dict.keys())\n",
    "for idx in range(0, num_images - 1,2): \n",
    "    '''\n",
    "    if idx > 1:\n",
    "        break\n",
    "    '''\n",
    "    left_img = sparse_img_dict[idx + 1]\n",
    "    right_img = sparse_img_dict[idx + 2]\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    rel_poses.append(rel_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f324a4-c9b4-4047-b092-60ebfdacdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
