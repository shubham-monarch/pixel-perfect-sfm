{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5983c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and refine it with the featuremetric optimization. We then localize an image downloaded from the Internet and show the effect of the refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c6f36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We start by defining some output paths: where the intermediate files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from hloc import extract_features, match_features, reconstruction, pairs_from_exhaustive, visualization\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "\n",
    "from pixsfm.util.visualize import init_image, plot_points2D\n",
    "from pixsfm.refine_hloc import PixSfM\n",
    "from pixsfm import ostream_redirect\n",
    "from PIL import Image, ImageDraw\n",
    "import pycolmap\n",
    "import numpy as np\n",
    "#import visualize_model\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b577c8a-72ac-48da-aac2-c4c2bd22c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab294072",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/monarch/')\n",
    "outputs = Path('outputs/monarch-demo/')\n",
    "!rm -rf $outputs\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "raw_dir = outputs / \"raw\"\n",
    "ref_dir = outputs / \"ref\"\n",
    "''' model location in case of intrinsics locked '''\n",
    "ref_dir_locked = outputs / \"ref_locked\"\n",
    "''' model location in case of intrinsics not locked '''\n",
    "ref_dir_not_locked = outputs / \"ref_dir_not_locked\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bef9e3",
   "metadata": {},
   "source": [
    "Here we will use SuperPoint local features with the SuperGlue matcher, but it's easy to switch to other features like SIFT or R2D2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc286cf",
   "metadata": {},
   "source": [
    "### Analysing Sparse Pointcloud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4768fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_dir = Path(\"/home/skumar/stereo_colmap_cli_output/sparse/\")\n",
    "print(f\"sparse_dir: {sparse_dir.as_posix()}\")\n",
    "sparse_images = sparse_dir / \"images.bin\"\n",
    "sparse_points3D = sparse_dir / \"points3D.bin\"\n",
    "sparse_cameras = sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7293679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = pycolmap.Reconstruction()\n",
    "sparse_model.read_binary(sparse_dir.as_posix())\n",
    "print(f\"sparse_model.summary(): {sparse_model.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51fd20",
   "metadata": {},
   "source": [
    "### Baseline check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "from read_write_model import read_images_binary \n",
    "sparse_img_dict = read_images_binary(sparse_images)\n",
    "print(sparse_img_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_extrinsics(img):\n",
    "    from read_write_model import qvec2rotmat\n",
    "    R = qvec2rotmat(img.qvec)\n",
    "    t = img.tvec.reshape(3,-1)\n",
    "    R_t = np.concatenate((R,t), axis = 1)\n",
    "    R_t = np.vstack([np.array([0,0,0,1]), R_t])\n",
    "    return R_t    #  4 * 4 matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d23781",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "e_lw => left camera pose in world frame (4 * 4)\n",
    "e_rw => right camera pose in world frame (4 * 4)\n",
    "'''\n",
    "def calculate_relative_pose(e_lw, e_rw): \n",
    "    from scipy.spatial.transform import Rotation\n",
    "    e_rl = e_rw * np.linalg.inv(e_lw) #right camera in the frame of the left camera\n",
    "    R = e_rl[:3,:3] #extracting the rotation matrix\n",
    "    dx = e_rl[0,3]\n",
    "    dy = e_rl[1,3]\n",
    "    dz = e_rl[2,3]\n",
    "    dquat = Rotation.from_matrix(R).as_quat()\n",
    "    #rel_pose =  [dx, dy] + dquat\n",
    "    rel_pose = [dx,dy,dz]\n",
    "    for q in dquat: \n",
    "        rel_pose.append(q)\n",
    "    return rel_pose\n",
    "    #return [dx,dy]\n",
    "    #print(f\"dx: {dx} dy: {dy} dquat: {dquat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57333f-c5dd-4800-8c5c-41bccec93eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"type(sparse_images): {type(sparse_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_poses = []\n",
    "num_images = len(sparse_img_dict.keys())\n",
    "for idx in range(0, num_images - 1,2): \n",
    "    '''\n",
    "    if idx > 1:\n",
    "        break\n",
    "    '''\n",
    "    left_img = sparse_img_dict[idx + 1]\n",
    "    right_img = sparse_img_dict[idx + 2]\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    rel_poses.append(rel_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f324a4-c9b4-4047-b092-60ebfdacdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4799f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(24)\n",
    "df = pd.DataFrame({'A': np.linspace(1, 10, 10)})\n",
    "df = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list('BCDE'))],\n",
    "               axis=1)\n",
    "df.iloc[0, 2] = np.nan\n",
    "df.style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57fbbf",
   "metadata": {},
   "source": [
    "# Localization\n",
    "Now that we have a 3D map of the scene, we can localize any image. To demonstrate this, we download [a night-time image from Wikimedia](https://commons.wikimedia.org/wiki/File:Paris_-_Basilique_du_Sacr%C3%A9_Coeur,_Montmartre_-_panoramio.jpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://upload.wikimedia.org/wikipedia/commons/5/53/Paris_-_Basilique_du_Sacr%C3%A9_Coeur%2C_Montmartre_-_panoramio.jpg\"\n",
    "# try other queries by uncommenting their url\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/8/8e/Sacr%C3%A9_C%C5%93ur_at_night%21_%285865355326%29.jpg\"\n",
    "# url = \"https://upload.wikimedia.org/wikipedia/commons/c/c0/La_basilique_du_Sacr%C3%A9-Coeur_au_cr%C3%A9puscule_%28Paris%29_%284147593805%29.jpg\"\n",
    "query = 'query/night.jpg'\n",
    "!mkdir -p $images/query && wget $url -O $images/$query -q\n",
    "plot_images([read_image(images / query)], dpi=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854edc74",
   "metadata": {},
   "source": [
    "Again, we extract features for the query and match them exhaustively with all mapping images that were successfully reconstructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_registered = [refined.images[i].name for i in refined.reg_image_ids()]\n",
    "extract_features.main(feature_conf, images, image_list=[query], feature_path=features, overwrite=True)\n",
    "pairs_from_exhaustive.main(loc_pairs, image_list=[query], ref_list=references_registered)\n",
    "match_features.main(matcher_conf, loc_pairs, features=features, matches=matches, overwrite=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa75646",
   "metadata": {},
   "source": [
    "We read the EXIF data of the query to infer a rough initial estimate of camera parameters like the focal length. Then we estimate the absolute camera pose using PnP+RANSAC and refine the camera parameters. Under the hood, the `QueryLocalizer` takes care of extracting dense features for the query and runs the keypoint and pose adjustments, QKA and QBA. The refinement refines the camera parameters in-place so we can inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68b8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycolmap\n",
    "from pixsfm.localize import QueryLocalizer, pose_from_cluster\n",
    "\n",
    "camera = pycolmap.infer_camera_from_image(images / query)\n",
    "ref_ids = [refined.find_image_with_name(r).image_id for r in references_registered]\n",
    "conf = {\n",
    "    \"dense_features\": sfm.conf.dense_features,  # same features as the SfM refinement\n",
    "    \"PnP\": {  # initial pose estimation with PnP+RANSAC\n",
    "        'estimation': {'ransac': {'max_error': 12.0}},\n",
    "        'refinement': {'refine_focal_length': True, 'refine_extra_params': True},\n",
    "    },\n",
    "    \"QBA\": {  # query pose refinement\n",
    "        \"optimizer:\": {'refine_focal_length': True, 'refine_extra_params': True},\n",
    "    }\n",
    "}\n",
    "dense_features = sfm_outputs[\"feature_manager\"]\n",
    "localizer = QueryLocalizer(refined, conf, dense_features=dense_features)\n",
    "ret, log = pose_from_cluster(localizer, query, camera, ref_ids, features, matches, image_path=images/query)\n",
    "\n",
    "print(f'found {sum(ret[\"inliers\"])}/{len(ret[\"inliers\"])} inlier correspondences.')\n",
    "visualization.visualize_loc_from_log(images, query, log, refined, top_k_db=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f23aa",
   "metadata": {},
   "source": [
    "We visualize the correspondences between the query images a few mapping images. We can also visualize the estimated camera pose in the 3D map, shown here in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558eb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = pycolmap.Image(tvec=ret['tvec'], qvec=ret['qvec'])\n",
    "plot_camera_colmap(fig3d, pose, camera, color='rgba(128,128,255,0.5)', name=query, legendgroup=\"refined\")\n",
    "fig3d.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
