{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a5983c",
   "metadata": {},
   "source": [
    "In this notebook, we will build a 3D map of a scene from a small set of images and refine it with the featuremetric optimization. We then localize an image downloaded from the Internet and show the effect of the refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c6f36",
   "metadata": {},
   "source": [
    "# Setup\n",
    "We start by defining some output paths: where the intermediate files will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tqdm, tqdm.notebook\n",
    "tqdm.tqdm = tqdm.notebook.tqdm  # notebook-friendly progress bars\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from hloc import extract_features, match_features, reconstruction, pairs_from_exhaustive, visualization\n",
    "from hloc.visualization import plot_images, read_image\n",
    "from hloc.utils.viz_3d import init_figure, plot_points, plot_reconstruction, plot_camera_colmap\n",
    "\n",
    "from pixsfm.util.visualize import init_image, plot_points2D\n",
    "from pixsfm.refine_hloc import PixSfM\n",
    "from pixsfm import ostream_redirect\n",
    "from PIL import Image, ImageDraw\n",
    "import pycolmap\n",
    "#import visualize_model\n",
    "# redirect the C++ outputs to notebook cells\n",
    "cpp_out = ostream_redirect(stderr=True, stdout=True)\n",
    "cpp_out.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b577c8a-72ac-48da-aac2-c4c2bd22c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_arch_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab294072",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = Path('datasets/monarch/')\n",
    "outputs = Path('outputs/monarch-demo/')\n",
    "!rm -rf $outputs\n",
    "sfm_pairs = outputs / 'pairs-sfm.txt'\n",
    "loc_pairs = outputs / 'pairs-loc.txt'\n",
    "features = outputs / 'features.h5'\n",
    "matches = outputs / 'matches.h5'\n",
    "raw_dir = outputs / \"raw\"\n",
    "ref_dir = outputs / \"ref\"\n",
    "''' model location in case of intrinsics locked '''\n",
    "ref_dir_locked = outputs / \"ref_locked\"\n",
    "''' model location in case of intrinsics not locked '''\n",
    "ref_dir_not_locked = outputs / \"ref_dir_not_locked\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bef9e3",
   "metadata": {},
   "source": [
    "Here we will use SuperPoint local features with the SuperGlue matcher, but it's easy to switch to other features like SIFT or R2D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4182c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_conf = extract_features.confs['superpoint_aachen']\n",
    "matcher_conf = match_features.confs['superglue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29335da0",
   "metadata": {},
   "source": [
    "# 3D mapping and refinement\n",
    "First we list the images used for mapping. These are all day-time shots of Sacre Coeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de7d45-8e7a-48b3-a848-b76bbad407da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''masking of the tractor hood from the images '''\n",
    "# ''' output => datasets/monarch/{target_folder}/image_name.jpg '''\n",
    "# def draw_box_around_tractor_hood(image_path, target_folder): \n",
    "#     image = Image.open(image_path)\n",
    "#     w, h = image.size\n",
    "#     box_x1, box_y1 = 460, 770  # Top-left corner\n",
    "#     box_x2, box_y2 = 1630, 1080  # Bottom-right corner\n",
    "#     outline_color = (0, 0, 0)  # Red in RGB format\n",
    "#     fill_color = (0, 0, 0)  # Black in RGB format\n",
    "#     draw = ImageDraw.Draw(image)\n",
    "#     draw.rectangle([box_x1, box_y1, box_x2, box_y2], outline=outline_color, fill=fill_color)\n",
    "#     directory_path,filename = os.path.split(image_path)\n",
    "#     parent_directory_path = os.path.dirname(directory_path)\n",
    "#     target_directory = os.path.join(parent_directory_path, target_folder)\n",
    "#     os.makedirs(target_directory, exist_ok = True)\n",
    "#     target_image_path = os.path.join(target_directory,filename)\n",
    "#     image.save(target_image_path)\n",
    "#     return target_image_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe8e94-ca6f-4420-8e62-87a7670f9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_left = [str(p.relative_to(images)) for i, p in enumerate((images / 'left/').iterdir())]\n",
    "references_right = [str(p.relative_to(images)) for i, p in enumerate((images / 'right/').iterdir())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa6864-1d3e-47b7-992b-6797cc6faf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(references_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d23a0-b8f2-4fe7-b607-f46baca391e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_left = sorted(references_left, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "references_right = sorted(references_right, key=lambda x: int(x.split('/')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8915af4-57e6-4eef-bd6c-e773171e2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(references_left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a11524d-31b8-4cec-9092-2de82fd1d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "references_left = references_left[40:82] \n",
    "references_right = references_right[40:82]\n",
    "references = references_left + references_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362270f3-a77e-47f6-adec-7e1b1a0c8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sorting references so that each stereo pair is together in the list '''\n",
    "references = sorted(references, key=lambda x: int(x.split('/')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7620aa-6fe9-4366-8f4e-8a7e15891aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2b029-23b7-44a0-9f7c-20dd5d7e5f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' masking the tractor hood in all the images'''\n",
    "# ''' returns list of path to the masked images '''\n",
    "# start_time = time.time()\n",
    "# target_folder = \"masked_images\"\n",
    "# masked_references = [draw_box_around_tractor_hood(p, target_folder) for p in references]\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "\n",
    "# target_path = os.path.join(images, target_folder)\n",
    "\n",
    "# ''' sorting masked_references sequentially '''\n",
    "# ''' smf => sorted masked references '''\n",
    "# #smf = sorted(masked_references, key = lambda x: int(((x.split(\"/\")[-1]).split(\".\")[0]).split(\"_\")[0]))\n",
    "\n",
    "# print(f\"type(masked_references): {type(masked_references)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smf = []\n",
    "# for i in range(0, len(references)//2 - 1): \n",
    "#     left  = \"masked_images/\" + str(i) + \"_left.jpg\"\n",
    "#     right = \"masked_images/\" + str(i) + \"_right.jpg\"\n",
    "#     smf.append(left)\n",
    "#     smf.append(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"smf: {smf}\")\n",
    "# print(f\"len(smf) : {len(smf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e1654-5661-4ae3-9de4-4254257c6cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8d631-c4da-4ac0-b469-507c13528f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "references[59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34abc54c-1024-4608-b0ed-7c35951c3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path_ = extract_features.main(feature_conf, images, image_list= references, feature_path=features)\n",
    "#match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937f7d9-92c6-4a75-aaec-a3e4af511715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.extract_features import list_h5_names\n",
    "h5_feature_names = list_h5_names(features_path_)\n",
    "print(f\"len(h5_feature_names): {len(h5_feature_names)}\")\n",
    "print(h5_feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e86f9d",
   "metadata": {},
   "source": [
    "Then we extract features and match them across image pairs. Since we deal with few images, we simply match all pairs exhaustively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21540f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_path_ = extract_features.main(feature_conf, images, image_list=references_final, feature_path=features)\n",
    "# #match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9158e3-dad2-4197-bc21-e84e273b52c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc.utils.viz import plot_keypoints, save_plot\n",
    "from hloc.utils.io import get_keypoints\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "ref_trim_ = references[:4]\n",
    "plot_images([read_image(images / r) for r in ref_trim_], dpi=50, figsize=4.2)\n",
    "\n",
    "kps_list_ = [] \n",
    "for r in ref_trim_:\n",
    "    kps = get_keypoints(features_path_, r)\n",
    "    print(type(kps))\n",
    "    kps_list_.append(kps)\n",
    "    \n",
    "plot_keypoints(kps_list_, colors = \"red\",  ps = 10)\n",
    "\n",
    "current_path_ = os.getcwd()\n",
    "\n",
    "print(\"current_path: \", current_path_)\n",
    "\n",
    "print(type(current_path_))\n",
    "\n",
    "final_path = current_path_ + \"/kps.png\"\n",
    "\n",
    "\n",
    "save_plot(final_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7609a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc as collections\n",
    "isinstance(references, collections.Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ee806-24b4-43ca-a2fc-814f1677a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_from_exhaustive.stereo_main(sfm_pairs, image_list=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d90198-7012-4d2a-9158-841c889e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"features: \", features)\n",
    "#print(\"matches: \", matches)\n",
    "match_features.main(matcher_conf, sfm_pairs, features=features, matches=matches);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d33a9-0aca-465d-bef1-6a233768a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_names = list_h5_names(matches)\n",
    "for name in match_names: \n",
    "    if \"right-52.jpg\" in name: \n",
    "        print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' script to plot matches between two frames'''\n",
    "from hloc.utils.viz import plot_matches\n",
    "from hloc.utils.io import get_matches, get_keypoints\n",
    "#img1 = images.joinpath(references[0])\n",
    "#img2 = images.joinpath(references[1])\n",
    "\n",
    "#print(f\"img1 : {img1.as_posix()} img_2: {img2.as_posix()}\")\n",
    "\n",
    "print(f\"features: {features}\")\n",
    "kp1 = get_keypoints(features, references[0])\n",
    "kp2 = get_keypoints(features, references[1])\n",
    "print(f\"kp1.shape: {kp1.shape}\")\n",
    "\n",
    "m, _ = get_matches(matches, references[0], references[1])\n",
    "print(f\"m.shape: {m.shape}\")\n",
    "\n",
    "m1 = np.array([kp1[i] for i in m[:,0]])\n",
    "m2 = np.array([kp2[i] for i in m[:, 1]])\n",
    "\n",
    "#print(m1[:10])\n",
    "\n",
    "plot_images([read_image(images / r) for r in references[:2]], dpi=50, figsize=4.2)\n",
    "#plot_matches(kp1.transpose, kp2.transpose)\n",
    "#plot_matches(kp1.transpose, kp2.transpose)\n",
    "plot_matches(m1, m2)\n",
    "#plot_matches(m[:,0], m[:,1])\n",
    "#print(m[:10])\n",
    "#kp1 = \n",
    "#matches, scores = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232823a",
   "metadata": {},
   "source": [
    "Now we run the reconstruction with and without the featuremetric refinement. For this dataset, when computing the dense features, we resize the images such that they are not larger than 1024 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ef73b-b4dc-49cb-be9a-d4d104ec13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = 1093.2768\n",
    "fy = 1093.2768\n",
    "cx = 964.989\n",
    "cy = 569.276\n",
    "opencv_camera_params =','.join(map(str, (fx, fy, cx, cy, 0, 0, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0a949-5ff9-4053-ac91-0446a8c8ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sfm = PixSfM({\"dense_features\": {\"max_edge\": 1024}})\n",
    "\n",
    "\n",
    "#conf1 = {\"dense_features\": {\"max_edge\": 1024}}\n",
    "\n",
    "conf2 = {\n",
    "    \"BA\": {\"optimizer\": {\"refine_focal_length\": False,\"refine_extra_params\": False, \"refine_extrinsics\": False}},\n",
    "    \"dense_features\": {\"max_edge\":1024}\n",
    "}\n",
    "\n",
    "sfm = PixSfM(conf=conf2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f58135-114b-47c8-9570-eeb4367e9924",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''CASE 2 => INITIAL K IS PROVIDED + K IS LOCKED '''\n",
    "\n",
    "image_options = dict(camera_model='OPENCV', \n",
    "                     camera_params=opencv_camera_params\n",
    "                    )\n",
    "\n",
    "mapper_options_one = dict(ba_refine_focal_length=False, \n",
    "                      ba_refine_extra_params=False,\n",
    "                     ba_refine_principal_point=False)\n",
    "\n",
    "mapper_options_two = dict(ba_refine_focal_length=False, \n",
    "                      ba_refine_extra_params=False,\n",
    "                     ba_refine_principal_point=False)\n",
    "\n",
    "hloc_args_not_locked = dict(image_list=references,\n",
    "                image_options=image_options,\n",
    "                camera_mode=\"PER_FOLDER\",\n",
    "                mapper_options=mapper_options_two)\n",
    "\n",
    "#hloc_args_not_locked = dict(image_list=references)\n",
    "\n",
    "K_locked, sfm_outputs_not_locked = sfm.reconstruction(ref_dir_locked, images, sfm_pairs, features, matches, **hloc_args_not_locked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708a98e-2ea9-4808-bab6-a31983e21060",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "e_lw => left camera pose in world frame (4 * 4)\n",
    "e_rw => right camera pose in world frame (4 * 4)\n",
    "'''\n",
    "#def calculate_relative_pose(e_lw, e_rw):\n",
    "def calculate_relative_pose(e_lw: np.ndarray, e_rw: np.ndarray):\n",
    "    #print(f\"Inside the calculate_relative_pose function\")\n",
    "    from scipy.spatial.transform import Rotation\n",
    "    e_wl = np.linalg.inv(e_lw)\n",
    "    #print(f\"e_wl: {e_wl}\")\n",
    "    #e_rl = e_rw * np.linalg.inv(e_lw) #right camera in the frame of the left camera\n",
    "    #e_rl = e_rw * e_wl #right camera in the frame of the left camera\n",
    "    #print(f\"e_rl: {e_rl}\")\n",
    "    e_rl = np.dot(e_rw,np.linalg.inv(e_lw))\n",
    "    R = e_rl[:3,:3] #extracting the rotation matrix\n",
    "    dx = e_rl[0,3]\n",
    "    dy = e_rl[1,3]\n",
    "    dz = e_rl[2,3]\n",
    "    dquat = Rotation.from_matrix(R).as_quat()\n",
    "    #rel_pose =  [dx, dy] + dquat\n",
    "    rel_pose = [dx,dy,dz]\n",
    "    for q in dquat: \n",
    "        rel_pose.append(q)\n",
    "    return rel_pose\n",
    "    #return [dx,dy]\n",
    "    #print(f\"dx: {dx} dy: {dy} dquat: {dquat}\")\n",
    "\n",
    "\n",
    "def cam_extrinsics(img):\n",
    "    from read_write_model import qvec2rotmat\n",
    "    R = qvec2rotmat(img.qvec)\n",
    "    t = img.tvec.reshape(3,-1)\n",
    "    #print(f\"R: {R} t: {t}\")\n",
    "    R_t = np.concatenate((R,t), axis = 1)\n",
    "    #R_t = np.vstack([np.array([0,0,0,1]), R_t])\n",
    "    R_t = np.vstack([R_t, np.array([0,0,0,1])])\n",
    "    return R_t    #  4 * 4 matrix\n",
    "\n",
    "def calculate_relative_pose_between(left_idx: int, right_idx: int):\n",
    "    left_img = sparse_img_dict[left_idx]\n",
    "    right_img = sparse_img_dict[right_idx]\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    return rel_pose\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77860966-409a-45fb-9df0-28f0a3794f69",
   "metadata": {},
   "source": [
    "#### Camera positions WITHOUT Rig Bundle Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b074e-76b1-480c-af86-c4aa3ddefff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#sparse_dir = Path(\"/home/skumar/stereo_colmap_cli_output/sparse/\")\n",
    "#sparse_dir = ref_dir_locked / \"hloc\"\n",
    "sparse_dir = Path(\"/home/skumar/stereo_colmap_cli_output/\")\n",
    "print(f\"sparse_dir: {sparse_dir.as_posix()}\")\n",
    "sparse_images = sparse_dir / \"images.bin\"\n",
    "sparse_points3D = sparse_dir / \"points3D.bin\"\n",
    "sparse_cameras = sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ff935-5e51-4f96-8502-851a5b3ad467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "from read_write_model import read_images_binary \n",
    "sparse_img_dict = read_images_binary(sparse_images)\n",
    "print(f\"{len(sparse_img_dict.keys())} ==> {sparse_img_dict.keys()}\")\n",
    "print(f\"min_key: {min(sparse_img_dict.keys())} mx_key: {max(sparse_img_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9239668-3246-4c31-b5de-3cc014afe1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_extrinsics(sparse_img_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b633f78d-7fba-4d8d-8ab1-cd445552e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rel_poses = []\n",
    "num_images = len(sparse_img_dict.keys())\n",
    "for idx in range(1, num_images // 2 + 1):\n",
    "    left_img = sparse_img_dict[idx]\n",
    "    right_img = sparse_img_dict[idx + 42]\n",
    "    #print(f\"left_img_name: {left_img.name} right_img_name: {right_img.name}\")\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    e_rl = calculate_relative_pose(e_lw, e_rw)\n",
    "    rel_poses.append(e_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34d636-d470-4273-8c9c-258d08e313a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088c5d9-f2be-4e8c-92e8-15390d6dd25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dr = np.hstack((np.array(df['dx']).reshape(-1,1), np.array(df['dy']).reshape(-1,1), np.array(df['dz']).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e9592-006f-4933-8ad6-4373b396b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c85e6-6e0e-44ee-9793-3f1dfd56a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linalg.norm(dr, axis=1, ord=2)\n",
    "# plt.hist(x, 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cc0fa-25ee-4136-942d-3a9422a15913",
   "metadata": {},
   "source": [
    "#### Camera poses with Rig Bundle Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ceb78-de4c-4db3-bdfb-5cb8add43f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rig_ba_sparse_dir = Path(\"/home/skumar/rig_dense/sparse/\")\n",
    "print(f\"rig_ba_sparse_dir: {rig_ba_sparse_dir.as_posix()}\")\n",
    "rig_ba_sparse_images = rig_ba_sparse_dir / \"images.bin\"\n",
    "rig_ba_sparse_points3D = rig_ba_sparse_dir / \"points3D.bin\"\n",
    "rig_ba_sparse_cameras = rig_ba_sparse_dir / \"cameras.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f269c61-bfc7-41f8-8dac-1148628ec948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/skumar/colmap/scripts/python\")\n",
    "from read_write_model import read_images_binary \n",
    "rig_ba_sparse_img_dict = read_images_binary(rig_ba_sparse_images)\n",
    "print(f\"{len(rig_ba_sparse_img_dict.keys())} => {rig_ba_sparse_img_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f38e7a-2547-4b16-b6a2-a25dee4b71e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rig_ba_rel_poses = []\n",
    "num_images = len(rig_ba_sparse_img_dict.keys())\n",
    "for idx in range(1, num_images // 2 + 1):\n",
    "    left_img = rig_ba_sparse_img_dict[idx]\n",
    "    right_img = rig_ba_sparse_img_dict[idx + 42]\n",
    "    if idx < 5:\n",
    "        print(f\"left_img_name: {left_img.name} right_img_name: {right_img.name}\")\n",
    "    e_lw = cam_extrinsics(left_img)  #left camera pose w.r.t. world\n",
    "    e_rw = cam_extrinsics(right_img) #right camera pose w.r.t world\n",
    "    rel_pose = calculate_relative_pose(e_lw, e_rw)\n",
    "    rig_ba_rel_poses.append(rel_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45309f8b-af8f-417b-b928-a44ab79c30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "df = pd.DataFrame(rig_ba_rel_poses, columns=['dx', 'dy', 'dz', 'qx' , 'qy', 'qz' , 'qw'])\n",
    "df.style"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
